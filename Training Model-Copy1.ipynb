{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:40: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:43: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/gmo/.local/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/gmo/.local/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:46: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /home/gmo/Documents/EE240/FinalProject/agent.py:47: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmo/.local/lib/python3.5/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Frame 233 - Frames/sec 139.0 - Epsilon 0.9999 - Mean Reward 2.665236051502146\n",
      "Episode 100 - Frame 43537 - Frames/sec 143.0 - Epsilon 0.9892 - Mean Reward 3.1668371964098183\n",
      "Episode 200 - Frame 85520 - Frames/sec 142.0 - Epsilon 0.9788 - Mean Reward 3.163144960939203\n",
      "Episode 300 - Frame 129298 - Frames/sec 84.0 - Epsilon 0.9682 - Mean Reward 3.1623640414451097\n",
      "Episode 400 - Frame 166310 - Frames/sec 70.0 - Epsilon 0.9593 - Mean Reward 3.409637260533143\n",
      "Episode 500 - Frame 204562 - Frames/sec 69.0 - Epsilon 0.9501 - Mean Reward 3.5032590715005165\n",
      "Episode 600 - Frame 245780 - Frames/sec 69.0 - Epsilon 0.9404 - Mean Reward 3.2047334949284334\n",
      "Episode 700 - Frame 286977 - Frames/sec 69.0 - Epsilon 0.9308 - Mean Reward 3.2198258884503734\n",
      "Episode 800 - Frame 324685 - Frames/sec 70.0 - Epsilon 0.922 - Mean Reward 3.467922920466985\n",
      "Episode 900 - Frame 368352 - Frames/sec 71.0 - Epsilon 0.912 - Mean Reward 3.26133360310438\n",
      "Episode 1000 - Frame 404003 - Frames/sec 70.0 - Epsilon 0.9039 - Mean Reward 3.3581039401270214\n",
      "Episode 1100 - Frame 447246 - Frames/sec 70.0 - Epsilon 0.8942 - Mean Reward 3.3409185760360653\n",
      "Episode 1200 - Frame 488852 - Frames/sec 70.0 - Epsilon 0.885 - Mean Reward 3.4065693743633476\n",
      "Episode 1300 - Frame 521405 - Frames/sec 70.0 - Epsilon 0.8778 - Mean Reward 3.656807994308787\n",
      "Episode 1400 - Frame 558049 - Frames/sec 70.0 - Epsilon 0.8698 - Mean Reward 3.740507275679187\n",
      "Episode 1500 - Frame 597381 - Frames/sec 70.0 - Epsilon 0.8613 - Mean Reward 3.4463885549845292\n",
      "Episode 1600 - Frame 631146 - Frames/sec 70.0 - Epsilon 0.854 - Mean Reward 3.5494429913030814\n",
      "Episode 1700 - Frame 675343 - Frames/sec 70.0 - Epsilon 0.8446 - Mean Reward 3.4877024104342467\n",
      "Episode 1800 - Frame 706087 - Frames/sec 71.0 - Epsilon 0.8382 - Mean Reward 3.9593640913452477\n",
      "Episode 1900 - Frame 741141 - Frames/sec 69.0 - Epsilon 0.8309 - Mean Reward 3.806011176728112\n",
      "Episode 2000 - Frame 780228 - Frames/sec 69.0 - Epsilon 0.8228 - Mean Reward 3.6775520989205126\n",
      "Episode 2100 - Frame 809771 - Frames/sec 69.0 - Epsilon 0.8167 - Mean Reward 3.704720289409286\n",
      "Episode 2200 - Frame 836408 - Frames/sec 69.0 - Epsilon 0.8113 - Mean Reward 3.853013727977164\n",
      "Episode 2300 - Frame 865503 - Frames/sec 69.0 - Epsilon 0.8054 - Mean Reward 3.9461396902728945\n",
      "Episode 2400 - Frame 896239 - Frames/sec 70.0 - Epsilon 0.7993 - Mean Reward 3.9988960656874615\n",
      "Episode 2500 - Frame 922793 - Frames/sec 70.0 - Epsilon 0.794 - Mean Reward 3.78969275336657\n",
      "Episode 2600 - Frame 956714 - Frames/sec 70.0 - Epsilon 0.7873 - Mean Reward 3.8795440414682765\n",
      "Episode 2700 - Frame 979751 - Frames/sec 69.0 - Epsilon 0.7828 - Mean Reward 4.307931820913271\n",
      "Episode 2800 - Frame 1013700 - Frames/sec 69.0 - Epsilon 0.7761 - Mean Reward 3.7003923834904073\n",
      "Episode 2900 - Frame 1038771 - Frames/sec 70.0 - Epsilon 0.7713 - Mean Reward 4.048266658455052\n",
      "Episode 3000 - Frame 1064731 - Frames/sec 69.0 - Epsilon 0.7663 - Mean Reward 4.270683197430684\n",
      "Episode 3100 - Frame 1085776 - Frames/sec 70.0 - Epsilon 0.7623 - Mean Reward 4.5414492675699965\n",
      "Episode 3200 - Frame 1112113 - Frames/sec 70.0 - Epsilon 0.7573 - Mean Reward 4.375410619476006\n",
      "Episode 3300 - Frame 1133726 - Frames/sec 70.0 - Epsilon 0.7532 - Mean Reward 4.668540546519563\n",
      "Episode 3400 - Frame 1156565 - Frames/sec 71.0 - Epsilon 0.7489 - Mean Reward 4.362971254371488\n",
      "Episode 3500 - Frame 1182640 - Frames/sec 69.0 - Epsilon 0.744 - Mean Reward 4.441934713067556\n",
      "Episode 3600 - Frame 1206015 - Frames/sec 69.0 - Epsilon 0.7397 - Mean Reward 4.253616727805094\n",
      "Episode 3700 - Frame 1226300 - Frames/sec 69.0 - Epsilon 0.736 - Mean Reward 4.821107156195647\n",
      "Episode 3800 - Frame 1249074 - Frames/sec 69.0 - Epsilon 0.7318 - Mean Reward 4.301414208418433\n",
      "Episode 3900 - Frame 1273672 - Frames/sec 69.0 - Epsilon 0.7273 - Mean Reward 4.399522576040303\n",
      "Episode 4000 - Frame 1299562 - Frames/sec 70.0 - Epsilon 0.7226 - Mean Reward 4.111622886432088\n",
      "Episode 4100 - Frame 1318952 - Frames/sec 69.0 - Epsilon 0.7191 - Mean Reward 4.780786278872713\n",
      "Episode 4200 - Frame 1344305 - Frames/sec 69.0 - Epsilon 0.7146 - Mean Reward 4.2663487803993005\n",
      "Episode 4300 - Frame 1365226 - Frames/sec 69.0 - Epsilon 0.7108 - Mean Reward 4.550018745668945\n",
      "Episode 4400 - Frame 1389646 - Frames/sec 69.0 - Epsilon 0.7065 - Mean Reward 4.307333653229836\n",
      "Episode 4500 - Frame 1408969 - Frames/sec 69.0 - Epsilon 0.7031 - Mean Reward 4.7354157087303\n",
      "Episode 4600 - Frame 1429366 - Frames/sec 69.0 - Epsilon 0.6995 - Mean Reward 4.737139021017535\n",
      "Episode 4700 - Frame 1448277 - Frames/sec 69.0 - Epsilon 0.6962 - Mean Reward 4.831361126106417\n",
      "Episode 4800 - Frame 1473570 - Frames/sec 69.0 - Epsilon 0.6918 - Mean Reward 4.766848153755716\n",
      "Episode 4900 - Frame 1499511 - Frames/sec 69.0 - Epsilon 0.6874 - Mean Reward 4.362254523846276\n",
      "Episode 5000 - Frame 1522409 - Frames/sec 69.0 - Epsilon 0.6834 - Mean Reward 4.87903394396275\n",
      "Episode 5100 - Frame 1542159 - Frames/sec 69.0 - Epsilon 0.6801 - Mean Reward 4.863272682376769\n",
      "Episode 5200 - Frame 1563119 - Frames/sec 69.0 - Epsilon 0.6765 - Mean Reward 4.834208811239223\n",
      "Episode 5300 - Frame 1586588 - Frames/sec 69.0 - Epsilon 0.6726 - Mean Reward 4.641473052912419\n",
      "Episode 5400 - Frame 1608204 - Frames/sec 70.0 - Epsilon 0.6689 - Mean Reward 4.707534286792124\n",
      "Episode 5500 - Frame 1628701 - Frames/sec 70.0 - Epsilon 0.6655 - Mean Reward 4.768532623609835\n",
      "Episode 5600 - Frame 1653094 - Frames/sec 69.0 - Epsilon 0.6615 - Mean Reward 4.732782094307152\n",
      "Episode 5700 - Frame 1670251 - Frames/sec 69.0 - Epsilon 0.6587 - Mean Reward 5.30757818804689\n",
      "Episode 5800 - Frame 1690568 - Frames/sec 60.0 - Epsilon 0.6553 - Mean Reward 4.930875706587895\n",
      "Episode 5900 - Frame 1710834 - Frames/sec 68.0 - Epsilon 0.652 - Mean Reward 4.99157627546035\n",
      "Episode 6000 - Frame 1730210 - Frames/sec 68.0 - Epsilon 0.6489 - Mean Reward 5.1825812964188955\n",
      "Episode 6100 - Frame 1750366 - Frames/sec 68.0 - Epsilon 0.6456 - Mean Reward 4.985577756849208\n",
      "Episode 6200 - Frame 1771992 - Frames/sec 68.0 - Epsilon 0.6421 - Mean Reward 4.931933423871965\n",
      "Episode 6300 - Frame 1790060 - Frames/sec 68.0 - Epsilon 0.6392 - Mean Reward 5.345105374138605\n",
      "Episode 6400 - Frame 1807136 - Frames/sec 68.0 - Epsilon 0.6365 - Mean Reward 5.457790351745633\n",
      "Episode 6500 - Frame 1825025 - Frames/sec 68.0 - Epsilon 0.6337 - Mean Reward 5.067532995040072\n",
      "Episode 6600 - Frame 1841828 - Frames/sec 69.0 - Epsilon 0.631 - Mean Reward 5.4627649990187015\n",
      "Episode 6700 - Frame 1858123 - Frames/sec 68.0 - Epsilon 0.6284 - Mean Reward 5.544414712006223\n",
      "Episode 6800 - Frame 1875281 - Frames/sec 68.0 - Epsilon 0.6257 - Mean Reward 5.606376768151737\n",
      "Episode 6900 - Frame 1892413 - Frames/sec 69.0 - Epsilon 0.6231 - Mean Reward 5.265415918695168\n",
      "Episode 7000 - Frame 1908658 - Frames/sec 69.0 - Epsilon 0.6205 - Mean Reward 5.426059379197356\n",
      "Episode 7100 - Frame 1927889 - Frames/sec 68.0 - Epsilon 0.6176 - Mean Reward 5.042388495887145\n",
      "Episode 7200 - Frame 1944220 - Frames/sec 69.0 - Epsilon 0.615 - Mean Reward 5.5710357189157245\n",
      "Episode 7300 - Frame 1960427 - Frames/sec 69.0 - Epsilon 0.6126 - Mean Reward 5.36652694188108\n",
      "Episode 7400 - Frame 1978018 - Frames/sec 69.0 - Epsilon 0.6099 - Mean Reward 5.526329699592352\n",
      "Episode 7500 - Frame 1997669 - Frames/sec 68.0 - Epsilon 0.6069 - Mean Reward 5.225248026466501\n",
      "Episode 7600 - Frame 2014615 - Frames/sec 68.0 - Epsilon 0.6043 - Mean Reward 5.372311062578485\n",
      "Episode 7700 - Frame 2031951 - Frames/sec 68.0 - Epsilon 0.6017 - Mean Reward 5.535643888311597\n",
      "Episode 7800 - Frame 2051645 - Frames/sec 68.0 - Epsilon 0.5987 - Mean Reward 5.506557911857948\n",
      "Episode 7900 - Frame 2071758 - Frames/sec 68.0 - Epsilon 0.5957 - Mean Reward 5.2520627842044965\n",
      "Episode 8000 - Frame 2090612 - Frames/sec 68.0 - Epsilon 0.5929 - Mean Reward 5.706346172684163\n",
      "Episode 8100 - Frame 2106812 - Frames/sec 67.0 - Epsilon 0.5905 - Mean Reward 5.536950800738957\n",
      "Episode 8200 - Frame 2125205 - Frames/sec 68.0 - Epsilon 0.5878 - Mean Reward 5.496675732869095\n",
      "Episode 8300 - Frame 2141252 - Frames/sec 68.0 - Epsilon 0.5855 - Mean Reward 5.7804308598931495\n",
      "Episode 8400 - Frame 2160983 - Frames/sec 68.0 - Epsilon 0.5826 - Mean Reward 5.2837380690494165\n",
      "Episode 8500 - Frame 2178950 - Frames/sec 68.0 - Epsilon 0.58 - Mean Reward 5.236806720156401\n",
      "Episode 8600 - Frame 2196254 - Frames/sec 67.0 - Epsilon 0.5775 - Mean Reward 5.605313464170376\n",
      "Episode 8700 - Frame 2214895 - Frames/sec 68.0 - Epsilon 0.5748 - Mean Reward 5.271806461909571\n",
      "Episode 8800 - Frame 2231457 - Frames/sec 68.0 - Epsilon 0.5724 - Mean Reward 5.386411100114964\n",
      "Episode 8900 - Frame 2248820 - Frames/sec 68.0 - Epsilon 0.57 - Mean Reward 5.375552747812769\n",
      "Episode 9000 - Frame 2267448 - Frames/sec 68.0 - Epsilon 0.5673 - Mean Reward 5.265635511936637\n",
      "Episode 9100 - Frame 2285608 - Frames/sec 63.0 - Epsilon 0.5647 - Mean Reward 5.081858685643329\n",
      "Episode 9200 - Frame 2303930 - Frames/sec 36.0 - Epsilon 0.5622 - Mean Reward 5.447237445284912\n",
      "Episode 9300 - Frame 2320414 - Frames/sec 36.0 - Epsilon 0.5598 - Mean Reward 5.7064700815007985\n",
      "Episode 9400 - Frame 2340430 - Frames/sec 36.0 - Epsilon 0.557 - Mean Reward 5.1938522163661585\n",
      "Episode 9500 - Frame 2355917 - Frames/sec 35.0 - Epsilon 0.5549 - Mean Reward 5.710409295971308\n",
      "Episode 9600 - Frame 2373584 - Frames/sec 36.0 - Epsilon 0.5524 - Mean Reward 5.4880912499046905\n",
      "Episode 9700 - Frame 2388138 - Frames/sec 36.0 - Epsilon 0.5504 - Mean Reward 5.8750178553707455\n",
      "Episode 9800 - Frame 2402757 - Frames/sec 36.0 - Epsilon 0.5484 - Mean Reward 5.904105148027075\n",
      "Episode 9900 - Frame 2419641 - Frames/sec 36.0 - Epsilon 0.5461 - Mean Reward 5.578799994682278\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from agent import DQNAgent\n",
    "from wrappers import wrapper\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Build env (first level, right only)\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = wrapper(env)\n",
    "\n",
    "# Parameters\n",
    "states = (84, 84, 4)\n",
    "actions = env.action_space.n\n",
    "\n",
    "# Agent\n",
    "agent = DQNAgent(states=states, actions=actions, max_memory=100000, double_q=True)\n",
    "\n",
    "# Episodes\n",
    "episodes = 10000\n",
    "rewards = []\n",
    "\n",
    "# Timing\n",
    "start = time.time()\n",
    "step = 0\n",
    "\n",
    "# Main loop\n",
    "for e in range(episodes):\n",
    "\n",
    "    # Reset env\n",
    "    state = env.reset()\n",
    "\n",
    "    # Reward\n",
    "    total_reward = 0\n",
    "    iter = 0\n",
    "\n",
    "    # Play\n",
    "    while True:\n",
    "\n",
    "        # Show env\n",
    "        # env.render()\n",
    "\n",
    "        # Run agent\n",
    "        action = agent.run(state=state)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, info = env.step(action=action)\n",
    "\n",
    "        # Remember\n",
    "        agent.add(experience=(state, next_state, action, reward, done))\n",
    "\n",
    "        # Replay\n",
    "        agent.learn()\n",
    "\n",
    "        # Total reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Increment\n",
    "        iter += 1\n",
    "\n",
    "        # If done break loop\n",
    "        if done or info['flag_get']:\n",
    "            \n",
    "            break\n",
    "\n",
    "    # Rewards\n",
    "    rewards.append(total_reward / iter)\n",
    "\n",
    "    # Print\n",
    "    if e % 100 == 0:\n",
    "        print('Episode {e} - '\n",
    "              'Frame {f} - '\n",
    "              'Frames/sec {fs} - '\n",
    "              'Epsilon {eps} - '\n",
    "              'Mean Reward {r}'.format(e=e,\n",
    "                                       f=agent.step,\n",
    "                                       fs=np.round((agent.step - step) / (time.time() - start)),\n",
    "                                       eps=np.round(agent.eps, 4),\n",
    "                                       r=np.mean(rewards[-100:])))\n",
    "        start = time.time()\n",
    "        step = agent.step\n",
    "\n",
    "# Save rewards\n",
    "np.save('rewards.npy', rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.replay(env,'./new_reward_models/',20, plot=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
